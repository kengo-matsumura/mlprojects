# Key learnings:

1. For classification problems, always look at the balance of majority and minority classes first.
   This can be done using a simple count plot.
   The metric trap occurs when using certain metrics to evaluate a classification model - using simple metrics like the
   accuracy score can be misleading. This is because we can achieve high accuracy by simplying 'predicting' the majority class
   for all cases, which would actually work most of the time!

2. When one-hot encoding nominal categorical variables, be aware of multicollinearity.
   For example, one-hot encoding a variable that contains the values 'blue', 'green' and 'white' will be highly correlated
   with each other. To avoid this, remove one of the dummy variables as the two remaining variables is enough
   to provide the same information, i.e. if 'green' is removed, then 'blue' = 0 and 'white' = 0 implies 'green'.
   The principle is similar to Principal Component Analysis (PCA) where you are trying to reduce dimensionality
   while still maintaining most of the information.
   
   Make sure to plot your data (such as using Seaborn's heatmap, or looking at a scatter plot of all variables against each
   other) and look at the correlation between features.
